{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b93fa4dd-58a5-40ba-a264-5bd981fd6432",
   "metadata": {},
   "source": [
    "# Time-Series Forecasting with Amazon SageMaker Autopilot using SageMaker Python SDK\n",
    "## Contents\n",
    "\n",
    "1. Introduction\n",
    "1. Setup\n",
    "1. Model Training\n",
    "1. Real-Time Predictions (Inference)\n",
    "1. Batch Predictions (Inference)\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "This notebook uses Amazon SageMaker Autopilot to train a time-series model and produce predictions against the trained model. At the top-level, customers fetch a set of tabular historical data from Amazon Redshift and convert the data into two csv files for training and inference and make an API call to train a model. Once the model has been trained, you can elect to produce prediction as a batch or via a real-time endpoint. As part of the training process, SageMaker Autopilot manages and runs multiple time series models concurrently. All of these models are combined into a single ensembled model which blends the candidate models in a ratio that minimizes forecast error. Customers are provided with metadata and models for the ensemble and all underlying candidate models too. SageMaker Autopilot orchestrates this entire process and provides several artifacts as a result.\n",
    "\n",
    "These artifacts include:\n",
    "\n",
    "1. backtest (holdout) forecasts per base model over multiple time windows,\n",
    "2. accuracy metrics per base model,\n",
    "3. backtest results and accuracy metrics for the ensembled model,\n",
    "4. a scaled explainability report displaying the importance of each covariate and static metadata feature.\n",
    "5. all model artifacts are provided as well on S3, which can be registered or use for batch/real-time inference\n",
    "\n",
    "## 2. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98139cc2-c2b5-44bf-ab76-44316424ed5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update boto3 using this method, or your preferred method\n",
    "!pip install --upgrade boto3 botocore --quiet\n",
    "!pip install --upgrade sagemaker --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648e8033-d47d-41dd-b5f8-066ea2ba9404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "from time import gmtime, strftime, sleep\n",
    "import datetime\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "session = sagemaker.Session()\n",
    "\n",
    "# Modify the following default_bucket to use a bucket of your choosing\n",
    "bucket = session.default_bucket()\n",
    "#bucket = 'my-bucket'\n",
    "prefix = 'sales-automl'\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "# This is the client we will use to interact with SageMaker Autopilot\n",
    "sm = boto3.Session().client(service_name=\"sagemaker\", region_name=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77649e08-065d-44d6-b992-5511e0a97935",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%sql project.redshift\n",
    "-- Fetch data from Amazon Redshift using below SQL Query\n",
    "select  sm.store_id, sm.store_name, dm.date_time as sales_date, ss.total_sales, sp.promo,sp.school_holiday\n",
    "from    store_dim sm,\n",
    "        date_dim dm,\n",
    "        store_sales ss,\n",
    "        store_promotions sp\n",
    "where   sm.store_id = ss.store_id\n",
    "and     dm.date_key = ss.date_key\n",
    "and     sm.store_id = sp.store_id\n",
    "and     dm.date_key = sp.date_key\n",
    "order by sm.store_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4205ef-d544-45d6-a820-57e508f11570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the dataframe using the output of the previous cell.\n",
    "df = _.df\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0280015f-164c-497f-bbaf-dc717690f874",
   "metadata": {},
   "source": [
    "We provide a sample set of data to accompany this notebook. You may use our synthetic dataset, or alter this notebook to accommodate your own data. As a note, the next cell will copy a file to your S3 bucket and prefix defined in the last cell. As an alternate, we provide a method to copy the file to your local disk too.\n",
    "\n",
    "IMPORTANT: When training a model, your input data can contain a mixture of covariate and static item metadata. Take care to create future-dated rows that extend to the end of your prediction horizon. In the future-dated rows, carry your static item metadata and expected covariate values. Future-dated target-value (y) should be empty. Please download the example synthetic file using the S3 copy command in the next cell. You can observe the data programmatically or in a text editor as an example.\n",
    "\n",
    "The structure of the CSV file provided is as follows:\n",
    "\n",
    "- **`store`**: unique ID for the store\n",
    "- **`saledate`**: datetime `YYYY-MM-DD HH:mm:ss`\n",
    "- **`sales`**: units of products sold that day\n",
    "- **`promo`**: was it a promotion day?\n",
    "- **`schoolholiday`**: was it a school holiday?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc62cc67-b873-4b8d-84ff-c8757cfd0b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull some data out for inference\n",
    "stores = df.store_id.unique()\n",
    "\n",
    "train_dfs = []\n",
    "inference_dfs = []\n",
    "for store in stores:\n",
    "    # Are there store with less than 25 entries in the datafraME?\n",
    "    try:\n",
    "        assert len(df[df.store_id==store]) >= 25\n",
    "    except:\n",
    "        print(f\"Store {store} has less than 25 entries\")\n",
    "        continue\n",
    "    # Create the inference dataset by taking the last 5 datapoints of each store\n",
    "    store_df = df[df.store_id==store].sort_values(by=\"sales_date\")\n",
    "    train_dfs.append(store_df[:-5])\n",
    "    inference_dfs.append(store_df[-5:])\n",
    "train_df = pd.concat(train_dfs)\n",
    "inference_df = pd.concat(inference_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdafa41-3c3d-4365-81b5-dcd48c97b2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the two dataframes from before into two csv's for training and inference\n",
    "train_df.to_csv(\"sales_train.csv\", index=False)\n",
    "inference_df.to_csv(\"sales_inference.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66a2ed4-619b-4c96-ba85-509688fa10cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of a DataFrame\n",
    "!head sales_train.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c519ff-ef82-4302-a62a-549d34dd006b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T17:55:49.873172Z",
     "iopub.status.busy": "2024-11-13T17:55:49.872565Z",
     "iopub.status.idle": "2024-11-13T17:55:49.876626Z",
     "shell.execute_reply": "2024-11-13T17:55:49.875818Z",
     "shell.execute_reply.started": "2024-11-13T17:55:49.873143Z"
    }
   },
   "source": [
    "## 3. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8dc3dd-fc35-4841-844f-0f3bdb9a838d",
   "metadata": {},
   "source": [
    "Establish an AutoML training job name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeefec7d-3067-4dfb-989d-e72c98d64ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"sales_train.csv\"\n",
    "item_identifier_attribute_name=\"store_id\"\n",
    "target_attribute_name=\"total_sales\"\n",
    "timestamp_attribute_name=\"sales_date\"\n",
    "columns_to_be_filled_with_zeros_if_missing = [\"promo\", \"school_holiday\"]\n",
    "\n",
    "\n",
    "base_job_name = \"sales-automl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d19efa-ccdc-4e1f-8f79-24ce55cf831e",
   "metadata": {},
   "source": [
    "Define training job specifications. More information about [create_auto_ml_job_v2](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker/client/create_auto_ml_job_v2.html) can be found in our SageMaker documentation.</n></n>This JSON body leverages the built-in sample data schema. Please consult the documentation to understand how to alter the parameters for your unique schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4713da2a-5bf7-4da9-a653-4e0ec860a749",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.automl.automlv2 import AutoMLTimeSeriesForecastingConfig, AutoMLV2, LocalAutoMLDataChannel\n",
    "\n",
    "input_data = LocalAutoMLDataChannel(\n",
    "    data_type=\"S3Prefix\",\n",
    "    channel_type=\"training\",\n",
    "    path=filename,\n",
    "    content_type=\"text/csv;header=present\"\n",
    ")\n",
    "\n",
    "ts_config = AutoMLTimeSeriesForecastingConfig(\n",
    "    forecast_frequency='D',  # The frequency of predictions in a forecast.\n",
    "    forecast_horizon=5,  # The number of time-steps that the model predicts.\n",
    "    forecast_quantiles=['p50','p60','p70','p80','p90'], # The quantiles used to train the model for forecasts at a specified quantile. \n",
    "    filling = {x: {'middlefill': 'zero', 'backfill' : 'zero', 'futurefill' : 'zero'} for x in columns_to_be_filled_with_zeros_if_missing},\n",
    "    item_identifier_attribute_name=item_identifier_attribute_name,\n",
    "    target_attribute_name=target_attribute_name,\n",
    "    timestamp_attribute_name=timestamp_attribute_name,\n",
    "    # grouping_attribute_names=['location_code']\n",
    ")\n",
    "\n",
    "automl_job = AutoMLV2(\n",
    "    problem_config=ts_config,\n",
    "    role=role,\n",
    "    base_job_name=base_job_name,\n",
    "    output_path=f's3://{bucket}/{prefix}/output'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cf5fbe-93b6-411d-8a31-367e49c6a79d",
   "metadata": {},
   "source": [
    "With parameters now defined, invoke the [training job] using Python SDK (https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html) and monitor for its completion. You can expect the training to take about 1 hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002ae00d-e9bf-4a2f-9865-2949d35e9e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "automl_job.fit(input_data, wait=True, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a352f632-253c-4ba7-ad29-2ffe114c249c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T00:30:58.878026Z",
     "iopub.status.busy": "2024-11-15T00:30:58.877331Z",
     "iopub.status.idle": "2024-11-15T00:30:58.881293Z",
     "shell.execute_reply": "2024-11-15T00:30:58.880635Z",
     "shell.execute_reply.started": "2024-11-15T00:30:58.877998Z"
    }
   },
   "source": [
    "Retrieve the best Candidate. Below is an example to use the best candidate in the subsequent inference phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bbfd9a-d02f-4dd0-a77c-385173b053e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_candidate= automl_job.best_candidate()\n",
    "best_candidate_name = best_candidate['CandidateName']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c6a16b-bc0c-46de-9a59-234abfd45df1",
   "metadata": {},
   "source": [
    "## 4. Deploy the best model to a SageMaker Real-time endpoint\n",
    "\n",
    "If you want to perform real-time inference, review this section. If you want to perform batch processing, you may skip the real-time inference section and move to Batch Predictions (Inference).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ffd2b4-91b6-4c88-b909-8ba33149151a",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = f\"ep-{best_candidate_name}-automl-ts\"\n",
    "\n",
    "automl_sm_model = automl_job.create_model(name=best_candidate_name, candidate=best_candidate)\n",
    "\n",
    "predictor = automl_job.deploy(initial_instance_count=1, endpoint_name=endpoint_name, instance_type='ml.m5.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70715d22-49c9-428c-8448-4003aa7ac1aa",
   "metadata": {},
   "source": [
    "## Now, we test the inference\n",
    "\n",
    "The next cells help demonstrate opening a local CSV file for inference. Alternately, this data could come from S3, a database query or live application. In this example, the data is loaded into a Python memory object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cbea35-1587-42e4-9539-5fb7a98e35df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting a very small sample data from the sales_inference.csv to test the realtime endpoint\n",
    "\n",
    "sales_inference_realtime = pd.read_csv(\"sales_inference.csv\")\n",
    "\n",
    "realtime_inference_test = sales_inference_realtime.sample(n=10)\n",
    "\n",
    "realtime_inference_test.to_csv(\"sales_realtime.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b726e56-8b66-4002-a7eb-b60011ea038b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A small sample file that corresponds to the sample training dataset and trained model schema\n",
    "\n",
    "input_file = './sales_realtime.csv'\n",
    "f=open(input_file,'r')\n",
    "inference_data = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfa7011-0011-4f6a-8241-9dd2b44add66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "\n",
    "realtime_predictor = Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    session = sagemaker.Session()\n",
    ")\n",
    "\n",
    "initial_args = {\n",
    "    \"EndpointName\": endpoint_name,\n",
    "    \"Body\": inference_data,\n",
    "    \"ContentType\": \"text/csv\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3072cf-636b-4682-9868-32ba195d3168",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = realtime_predictor.predict(\n",
    "    data=inference_data,\n",
    "    initial_args=initial_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d583d42-ddc2-415b-9770-3990b2d25224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoding the byte data to a string, assuming UTF-8 encoding\n",
    "decoded_data = response.decode('utf-8')\n",
    "\n",
    "output_file = 'real-time-prediction-output.csv'\n",
    "# Writing the decoded data to a CSV file\n",
    "with open(output_file, 'w', newline='') as file:\n",
    "    file.write(decoded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cc54a6-ee9e-48a9-b2d0-3637060f2134",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(StringIO(decoded_data), sep=',')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82b7807-68cf-4b9f-a368-ea4dfc6866d0",
   "metadata": {},
   "source": [
    "## 5. Batch Inference with SageMaker Batch Transform\n",
    "\n",
    "Amazon SageMaker Batch Transform is a high-performance and scalable service designed for running batch predictions on large datasets. It allows users to easily transform data and make predictions by deploying machine learning models without the need to manage any infrastructure. This service is particularly useful for scenarios where you need to process a large amount of data in a batch manner, such as for generating predictions from a trained model on a schedule or in response to specific events. Batch Transform automatically manages the computing resources required, scales them to match the volume of data, and efficiently processes the data in batches, making it a cost-effective solution for batch inference needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33002af-95ff-47bf-a765-ecd976ecdb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.session import Session\n",
    "\n",
    "inference_file_name = \"sales_inference.csv\"\n",
    "\n",
    "# Create the model object\n",
    "model = automl_job.create_model(\"sales-automl\")\n",
    "\n",
    "# Upload inference data\n",
    "inference_data = Session().upload_data(\n",
    "    path=inference_file_name, \n",
    "    bucket=bucket, key_prefix=prefix+'/inference'\n",
    ")\n",
    "\n",
    "# Create the Transformer\n",
    "transformer = model.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.12xlarge',\n",
    "    output_path=f's3://{bucket}/{prefix}/batch_transform/output/',\n",
    "    max_payload=0,  # in MB\n",
    "    strategy='SingleRecord',\n",
    "    assemble_with='Line',\n",
    ")\n",
    "\n",
    "# Start the transform job\n",
    "transformer.transform(\n",
    "    data=inference_data,\n",
    "    content_type='text/csv;header=present',\n",
    "    split_type='None',\n",
    ")\n",
    "\n",
    "# Wait for the transform job to finish\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc37ac18-d887-41dd-9f69-a9d85cea75db",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = transformer.output_path\n",
    "output_file = output_path+inference_file_name+'.out'\n",
    "!aws s3 cp $output_file ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf8d4d6-c5e8-4541-ba40-11f62a29749c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(inference_file_name+'.out')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e70c472-0ec3-4eea-954b-3edea1d0067f",
   "metadata": {},
   "source": [
    "#### Cleanup Real-time Endpoint Resources\n",
    "\n",
    "As needed, you can stop the endpoint and related billing costs as follows. When you need the endpoint again, you can follow the deployment steps again. Ideally, at a future time, another newer model is trained and able to be deployed as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e29879-e7a0-4fff-9421-593960d0e67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "realtime_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99c14c7-1134-4eec-93f8-5f0263aae4c9",
   "metadata": {},
   "source": [
    "## 6. MLOps Pipeline\n",
    "\n",
    "To automate re-training, we would normally use SageMaker Pipelines. However, AutoMLV2 is currently bugged with SageMaker Pipelines. We suggest using AWS Step Functions to automate model retraining and deployment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
